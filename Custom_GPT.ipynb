{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht_eUw4Zyxlz",
        "outputId": "23e5dfc5-64fa-442c-f6b9-979b9505dc61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import Transformer\n",
        "from transformers import AutoTokenizer  # pip install transformers\n",
        "from utils import (\n",
        "    BATCH_SIZE,\n",
        "    BLOCK_SIZE,\n",
        "    DEVICE,\n",
        "    DROPOUT,\n",
        "    LEARNING_RATE,\n",
        "    NUM_EMBED,\n",
        "    NUM_HEAD,\n",
        "    NUM_LAYER,\n",
        "    MAX_ITER,\n",
        "    EVAL_INTER,\n",
        "    encode,\n",
        "    decode,\n",
        "    get_batch,\n",
        "    save_model_to_chekpoint,\n",
        "    estimate_loss,\n",
        ")\n",
        "\n",
        "# load model from checkpoint\n",
        "# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n",
        "\n",
        "# example to decode sequence\n",
        "# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n",
        "# max_new_tokens=20)[0].tolist()\n",
        "# print(decode(vocab=vocab, enc_sec=enc_sec))\n",
        "\n",
        "# raw data\n",
        "path_do_data = \"data/train_noi.txt\"\n",
        "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
        "# we use pretrained BERT tokenizer for performance improvements\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "# data_raw = data_raw[4000000:] # short dataset\n",
        "\n",
        "# train/val split\n",
        "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
        "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# train a new model\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    num_embed=NUM_EMBED,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    num_heads=NUM_HEAD,\n",
        "    num_layers=NUM_LAYER,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "# load model to GPU if available\n",
        "m = model.to(DEVICE)\n",
        "# print the number of parameters in the model\n",
        "print(\n",
        "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cggrH1gy14C",
        "outputId": "6b89543b-ad4a-4126-c43e-288c0f70883e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (54325 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with 89.48M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer takes the model's parameters and the learning rate as input,\n",
        "# and updates the parameters during the training process in order to\n",
        "# minimize the loss function.\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
        "#MAX_ITER = 500\n",
        "for step in range(MAX_ITER):\n",
        "\n",
        "    # every EVAL_INTER evaluate the loss on train and val sets\n",
        "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
        "        loss_train = estimate_loss(\n",
        "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        loss_val = estimate_loss(\n",
        "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # backward() method on the loss variable calculates the gradients \n",
        "    # of the loss with respect to the model's parameters.\n",
        "    loss.backward()\n",
        "    # step() method on the optimizer updates the model's parameters \n",
        "    # using the calculated gradients, in order to minimize the loss.\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuQZHg59y8Jc",
        "outputId": "200f2577-b5c1-404d-fc47-4aae23d5b098"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step          0 | train loss 10.7617 | val loss 10.7851\n",
            "step        500 | train loss 0.3862 | val loss 8.3055\n",
            "step       1000 | train loss 0.1628 | val loss 9.5971\n",
            "step       1500 | train loss 0.1379 | val loss 10.1851\n",
            "step       2000 | train loss 0.1324 | val loss 10.0830\n",
            "step       2500 | train loss 0.1219 | val loss 10.8053\n",
            "step       3000 | train loss 0.1253 | val loss 11.0654\n",
            "step       3500 | train loss 0.1147 | val loss 11.1153\n",
            "step       4000 | train loss 0.1104 | val loss 11.3658\n",
            "step       4500 | train loss 0.1136 | val loss 11.3534\n",
            "step       4999 | train loss 0.1147 | val loss 11.6401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoint\", epoch=step)"
      ],
      "metadata": {
        "id": "xMJ328cSy-nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate some output based on the context\n",
        "contexts = [torch.zeros((1, 1), dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[2000, 2003]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[3001,2000,2013]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[2836,5054]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[8927, 2443]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[1996,1997,4106]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[3785, 2138]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[2006, 9932]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[5461, 7778]], dtype=torch.long, device=DEVICE),\n",
        "            torch.tensor([[11365, 1010]], dtype=torch.long, device=DEVICE)\n",
        "            ]\n",
        "for context in contexts:\n",
        "  print(\"output:\",\n",
        "      decode(\n",
        "          enc_sec=m.generate(idx=context, max_new_tokens=50, block_size=BLOCK_SIZE)[0],\n",
        "          tokenizer=tokenizer,\n",
        "      )\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JqVguMpy_Qo",
        "outputId": "223902b3-e733-4ba5-98fb-b3392d2016f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: [PAD] image of a matchstick. as a casual user of chatgpt or another generative model, you may well have even less of an idea of what the initial training data consisted of. ask chatgpt where its data comes from, and it\n",
            "output: to is that they can be trained automatically and are simple and computationally feasible to use. in speech recognition, the hidden markov model would output a sequence of n - dimensional real - valued vectors ( with n being a small integer, such as 10 ),\n",
            "output: systems to from driverless cars will be perceived by a human being interacting with an artificial entity that closely ( though imperfectly ) resembles another human. unsupervised learning : a branch of machine learning which, as the name suggests, blends elements of both\n",
            "output: performancesentangles the underlying factors of variation that explain the observed data. free and open - source software feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. however, real\n",
            "output: essays included as thei could have only a slightly higher iq score than the average human being, or it could be vastly, unfathomably more intelligent, comparable to the difference in cognitive ability between an ant and nobel laureate roger penrose. association rule learning\n",
            "output: the of analysis of random phenomena. history a standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. a hypothesis is proposed for the statistical relationship between\n",
            "output: conditions because a connection patterns embossed in their filters. therefore, on a scale of connectivity and complexity, cnns are on the lower extreme. architecture convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization\n",
            "output: on ai experts. musk never got back to us. if he really believed what he ’ s saying, he should have. we should still be very worried if musk is wrong about when driverless cars are coming, naive about what it takes to build\n",
            "output: techniques statistical and complex optimization frameworks. the advancement of deep learning techniques has brought further life to the field of computer vision. the accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed\n",
            "output: ##vis, 2010s raj reddy was the first person to take on continuous speech recognition as a graduate student at stanford university in the late 1960s. previous systems required users to pause after each word. reddy's system issued spoken commands for playing chess. models, methods,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "print(\n",
        "    decode(\n",
        "        enc_sec=m.generate(idx=context, max_new_tokens=50, block_size=BLOCK_SIZE)[0],\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7H5ZUz70oQT",
        "outputId": "4317cf83-a4e8-4f8f-8b6a-08e61303a8cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD] and their related methods \" in order to \" understand and analyse actual phenomena \" with data. it uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. however, generative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cp2wVy5J8me9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}